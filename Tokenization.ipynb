{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ac63987",
   "metadata": {},
   "source": [
    "In Natural Language Processing (NLP), tokenization is the process of breaking down a piece of text into smaller, more manageable units called tokens. These tokens can be individual words, characters, or subwords, depending on the specific tokenization approach. The purpose of tokenization is to convert raw text into a format that machines can understand and process for tasks like text classification, sentiment analysis, and machine translation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a190de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./anaconda3/lib/python3.11/site-packages (3.6.3)\n",
      "Requirement already satisfied: click in ./anaconda3/lib/python3.11/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in ./anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex in ./anaconda3/lib/python3.11/site-packages (from nltk) (2020.11.13)\n",
      "Requirement already satisfied: tqdm in ./anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1b406a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"Hello welcome, to Aishwarya's LNP learning notebook.\n",
    "I am learning NLP! to become expert in AI/ML.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "601c6133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello welcome, to Aishwarya's LNP learning notebook.\n",
      "I am learning NLP! to become expert in AI/ML.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "642627bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "# https://www.nltk.org/api/nltk.tokenize.html\n",
    "# paragraph to sentence \n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d420eec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/aishwaryagopalakrishnan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29e74478",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=sent_tokenize(corpus) # it is dividing on .!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "143ac215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hello welcome, to Aishwarya's LNP learning notebook.\",\n",
       " 'I am learning NLP!',\n",
       " 'to become expert in AI/ML.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18bb5d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paragraph to words \n",
    "# Sentence to words\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aaff2fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Aishwarya',\n",
       " \"'s\",\n",
       " 'LNP',\n",
       " 'learning',\n",
       " 'notebook',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'NLP',\n",
       " '!',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'AI/ML',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94bda6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'welcome', ',', 'to', 'Aishwarya', \"'s\", 'LNP', 'learning', 'notebook', '.']\n",
      "['I', 'am', 'learning', 'NLP', '!']\n",
      "['to', 'become', 'expert', 'in', 'AI/ML', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc8c55cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b541a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Aishwarya',\n",
       " \"'\",\n",
       " 's',\n",
       " 'LNP',\n",
       " 'learning',\n",
       " 'notebook',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'NLP',\n",
       " '!',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'AI',\n",
       " '/',\n",
       " 'ML',\n",
       " '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus) # ', / and s are split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06bc777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8dba39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9cd5772d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Aishwarya',\n",
       " \"'s\",\n",
       " 'LNP',\n",
       " 'learning',\n",
       " 'notebook.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'NLP',\n",
       " '!',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'AI/ML',\n",
       " '.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus) # notebook and . are together. only the last . of the paragraph, it is a seperate word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59ffa1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
